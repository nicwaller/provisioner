# Architecture

This application was developed with a data-first mentality. Data is the public interface to the application and particular attention needs to be given to system boundaries. I believe this is the philosophy that underlies a well-known quote from Linus Torvalds. [[1]](https://en.wikiquote.org/wiki/Linus_Torvalds)

> I'm a huge proponent of designing your code around the data, rather than the other way around, and I think it's one of the reasons git has been fairly successful […] I will, in fact, claim that the difference between a bad programmer and a good one is whether he considers his code or his data structures more important. Bad programmers worry about the code. Good programmers worry about data structures and their relationships.


## Stages and Precedence

Given any particular configuration, this tool will always execute in a [linear order](#linear-order). My design goal was to execute the configuration in the exact same order as it appears in the configuration file, but that came into conflict with my choice to use JSON schema, so as a compromise I settled on stages and precedence. 

First, resources are grouped into ordered **stages**.  Then within each stage, resources are applied based on this **precedence** order:

1. Packages
1. Files
1. Observers


See the appendix on [Resource Ordering](#resource-ordering) for theory about configuration management and the choices that were made by some popular tools. 


## JSON as configuration language

I chose JSON because it's [boring](http://boringtechnology.club). JSON is a mature file format; it has been in use on the internet for nearly two decades now. But more important than age is the excellent ecosystem of tooling that supports the JSON format.

- JSON parsers and generators are available natively in most modern languages.
- Many third-party parsers are available for specializing on specific needs.
- The [JSON Schema](https://json-schema.org) validation format is now mature (version 7).
- Popular editors like VS Code and PyCharm support auto-completion and validation when a JSON schema is available.
- Developers love using [jq](https://stedolan.github.io/jq/download/) for ad-hoc exploration and manipulation of JSON files.
- It's easy to hire people to work with JSON because it's simple and well-known.

But why does it work for this project specifically?

An excellent developer experience means that it's easy to write and validate configurations, and useful feedback is available when errors are present. Editing JSON files by hand can be a pain, but when JSON schema is available it can become easy, perhaps even joyful. JSON schema means that the same validation engine is used while authoring configuration and also when executing it on the server. JSON schema also enables auto-completion in popular editors like JetBrains PyCharm and VS Code.

The idea of [shifting left](https://cloud.google.com/architecture/devops/devops-tech-shifting-left-on-security) has recently proven to be a useful practice for advancing security goals in a DevSecOps organization. To "shift left" means to address security concerns earlier in the development lifecyle. In the context of server configuration management, this usually means automated enforcement of policy, so working well with policy tools is an important goal. A configuration represented as JSON can be easily validated against policies in Rego using [Open Policy Agent](https://www.openpolicyagent.org).

JSON works well as an *intermediate* representation that can be generated by other tools. If people want to write a custom DSL in Ruby, or automatically generate configuration by automated inspection of a running server, then JSON is an easy format to target.

*Disadvantages*

- JSON doesn't work well with a [read-eval-print-loop (REPL)](https://en.wikipedia.org/wiki/Read–eval–print_loop). Having a REPL available can improve developer productivity; this is why popular languages like Python, Ruby, and Swift all have REPLs.
- The current version of JSON schema limits the expressiveness of JSON, such that it may be more challenging or impossible to achieve exactly the desired data model.
- JSON doesn't allow comments. JSON5 allows comments, but doesn't play very well with JSON schema. If comments are more desirable than JSON schema support, it would be easy to write configuration in JSON5 and then compile it to standard JSON.

*Rejected Alternatives*

- YAML is too complex and has too many ambiguous cases.
- A variety of other interesting [serialization formats](https://blog.mbedded.ninja/programming/serialization-formats/a-comparison-of-serialization-formats/) aren't quite right for declarative configuration.
- Turing-complete languages are not sufficiently constrained, which allows complexity to balloon. You could still use a Ruby DSL to generate JSON if you want.
- CSV and other tabular formats are common in [Advent of Code](https://adventofcode.com) because they're quick and easy to work with, but tabular formats aren't well suited to the variety of objects that a configuration system has to work with.
- I seriously considered INI files, but JSON won mostly because of JSON schema.

## Atomic Updates

Resources should be updated atomically, when possible, even if that makes the operation more costly.

A `file` resource has metadata attributes like owner, group, and mode that could be updated in-place in a series of steps. However, an interruption in the series of steps could potentially render a file unusable until the next run. For example, consider a file `index.php` owned by `alice:alice` and having file mode `0777`. The file is used by `www-data`. The operator wants to secure the file by changing the mode to `0640`; in order to for the file to remain readable by the webserver it's also necessary to update the owner to `alice:www-data`. A crash between the first and second operation would render the file unreadable until next time the configuration is applied. This operation can be made atomic by preparing a temporary file in a staging area _in the same filesystem_ with appropriate data and metadata, then moving it into place (ie. by using [rename()](https://man7.org/linux/man-pages/man2/rename.2.html)). The most reliable way to ensure the staging file is in the same filesystem is to create the staging file in the same directory as the target file. However, this introduces a new risk for processes that scan the contents of the directory and don't expect duplicate files (eg. apache sites-available); therefore it should be possible to change this behaviour on a per-resource basis.

## Distinct plan/apply phases

Having a `--dry-run` option is highly desirable for a tool that may perform dangerous operations like file replacement or deletion. It's also critical that the dry run behaviour reflects the final executed behaviour as accurately as possible. If dry-run is implemented with a separate code path, or as a flag during the apply phase, it's likely to drift and/or cases will be missed. Therefore, the code that applies changes should depend on outputs from the planning phase (much like Terraform). The stages would probably look like this:

1. Construct a list of "shadow" resource states as described by the configuration file.
2. Gather a list of "current" resource states by inspecting the environment.
3. Reconcile each current resource against its shadow to produce a list of 0..many actions.
4. Write the intended actions to a journal.
5a. If `--dry-run`, output the actions.
5b. If `--wet-run`, apply each action and output the results. Update the journal after completion of each action.

Partially inspired by https://www.gresearch.co.uk/article/in-praise-of-dry-run/

## Twelve-Factor App

I've attempted to embrace the design philosophy of the [twelve-factor app](https://12factor.net).

1. One codebase, one repo.
2. Dependencies are explicitly declared in requirements.txt and Gemfiles, isolated with Bundler and venv, and transitive dependencies are pinned with lock files.
3. TODO: configure using environment variables
4. N/A (no backing services)
5. There is a distinct build phase to create the zipapp. 
6. The process is stateless because it doesn't take responsibility for storing the configuration, it only reads and executes it.
7. N/A (no exposed ports)
8. This tool has nearly unlimited scalability because it runs as a process on each target server. (A worse option would be to have one process that contacts each server.)
9. The principle of disposability is embodied by fast start-up and idempotent resources. However, the tool is not totally crash-safe. The lack of a journalling system means that well-timed crashes could result in some steps not being executed, or stale files being left in the filesystem. 
10. Dev/prod parity is achieved using Test Kitchen with the EC2 provider to ensure the operating environment is maximally similar to the live deployment environment.
11. Logs are emitted to stdout, not to a file. This avoids having to worry about logrotate. It moves the responsibility for exporting logs to the process supervisor. (TODO: convert character stream into event stream encoded with JSON lines.)
12. TODO: Admin processes like validating input files should be available from the CLI. 

---

# Appendix

## Resource Ordering
When designing a configuration management system, one of the most impactful decisions is how to handle resource ordering. Many popular tools can be categorized as either **linear** or **graph-based**.

neither imply idempotence, necessary

### Linear Order
A linear configuration defines steps in an obvious and well-defined order. Shell scripts are a common example of linear configuration, and many other imperative languages like Python and Ruby can also be used for imperative configuration.

*Examples*

- Shell
- Python
- Chef recipe
- Chef run list

*Advantages*

- Text files are linear. Because text files are the essential tool of developers and operators, maintaining the on-screen visual order during execution supports the [Principle of Least Astonishment](https://en.wikipedia.org/wiki/Principle_of_least_astonishment)
- Most race conditions are avoided when linear order is guaranteed. Race conditions and other concurrency problems are very difficult to troubleshoot, so it's best to avoid them if possible.
- When combined with idempotent operations, results are highly reproducible and easy to troubleshoot. 

### Graph-Based
A graph-based configuration defines resources as graph nodes and the dependency relationships between them as graph edges. This generates a [partial order](https://en.wikipedia.org/wiki/Partially_ordered_set) which offers more flexibility than a linear order. However, this comes at the cost of higher complexity in both the tool, and in writing configurations for the tool.

*Examples*

- Puppet
- Terraform
- CloudFormation
- systemd
- Makefile

*Advantages*

- Performance is the key advantage of graph-based systems. Giving up the guarantee of linear order means that the tool has the freedom to process multiple resources concurrently, as long as the partial ordering constraints are still satisfied. When operations are mostly independent, and especially when operations spend a lot of time waiting (as with Terraform), the performance benefit can be immense.



